{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "threatened-damages",
   "metadata": {},
   "source": [
    "# SDND Project 4 - Behavior Cloning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-beginning",
   "metadata": {},
   "source": [
    "## 0. Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elder-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "apparent-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "# Read in csv file\n",
    "with open('./data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "        \n",
    "# Pop out header & split dataset\n",
    "samples.pop(0)\n",
    "train_samples, valid_samples = train_test_split(samples, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-management",
   "metadata": {},
   "source": [
    "## 1. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "senior-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wrapped-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(samples, batch_size=32):\n",
    "    \n",
    "    # Number of samples\n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    while 1:\n",
    "        \n",
    "        # Shuffle samples\n",
    "        random.shuffle(samples)\n",
    "        \n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            \n",
    "            # Split batches\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            images = []\n",
    "            measurements = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # Read in angles\n",
    "                steering_center = float(batch_sample[3])\n",
    "                correction = 0.1\n",
    "                steering_left = steering_center + correction\n",
    "                steering_right = steering_center - correction\n",
    "                \n",
    "                # Read in images\n",
    "                path = './data/IMG/'\n",
    "                img_center = cv2.imread(path + batch_sample[0].split('/')[-1])\n",
    "                img_left = cv2.imread(path + batch_sample[1].split('/')[-1])\n",
    "                img_right = cv2.imread(path + batch_sample[2].split('/')[-1])\n",
    "                \n",
    "                # Image & angle: center camera\n",
    "                images.append(img_center)\n",
    "                measurements.append(steering_center)\n",
    "                # Augmentation: center camera\n",
    "                images.append(cv2.flip(img_center, 1))\n",
    "                measurements.append(steering_center*-1.0)\n",
    "                \n",
    "                # Image & angle: left camera\n",
    "                images.append(img_left)\n",
    "                measurements.append(steering_left)\n",
    "                # Augmentation: left camera\n",
    "                images.append(cv2.flip(img_left, 1))\n",
    "                measurements.append(steering_left*-1.0)\n",
    "\n",
    "                # Image & angle: right camera\n",
    "                images.append(img_right)\n",
    "                measurements.append(steering_right)\n",
    "                # Augmentation: right camera\n",
    "                images.append(cv2.flip(img_right, 1))\n",
    "                measurements.append(steering_right*-1.0)\n",
    "            \n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(measurements)\n",
    "            \n",
    "            yield sklearn.utils.shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "victorian-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & valid generators\n",
    "batch_size = 32\n",
    "train_generator = Generator(train_samples, batch_size=batch_size)\n",
    "valid_generator = Generator(valid_samples, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-harris",
   "metadata": {},
   "source": [
    "## 2. Model Architecture - NVIDIA PilotNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hollow-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D, Dropout, BatchNormalization\n",
    "from keras.layers.convolutional import Convolution2D as Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "model = Sequential()\n",
    "\n",
    "# Preprocess sample\n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160, 320, 3)))\n",
    "model.add(Cropping2D(cropping=((70, 25), (0, 0))))\n",
    "\n",
    "# Conv1 + ELU + BN + Pooling\n",
    "model.add(Conv2D(24, (5, 5), strides=2, activation='elu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(strides=(2, 2), padding='same'))\n",
    "\n",
    "# Conv2 + ELU + BN + Pooling\n",
    "model.add(Conv2D(36, (5, 5), strides=2, activation='elu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(strides=(2, 2), padding='same'))\n",
    "\n",
    "# Conv3 + ELU + BN + Pooling\n",
    "model.add(Conv2D(48, (5, 5), strides=2, activation='elu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(strides=(2, 2), padding='same'))\n",
    "\n",
    "# Conv4 + ELU + BN + Pooling\n",
    "model.add(Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(strides=(2, 2), padding='same'))\n",
    "\n",
    "# Conv5 + ELU + BN + Pooling\n",
    "model.add(Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(strides=(2, 2), padding='same'))\n",
    "\n",
    "# Flatten + FC1 + BN + Dropout\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "# FC2 + BN + Dropout\n",
    "model.add(Dense(50))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "# FC3 + BN + Dropout\n",
    "model.add(Dense(10)) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "# Output\n",
    "model.add(Dense(1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-killer",
   "metadata": {},
   "source": [
    "## 3. Train & Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit: Generator\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history_object = model.fit_generator(train_generator, \\\n",
    "            steps_per_epoch=math.ceil(len(train_samples) / batch_size), \\\n",
    "            validation_data=valid_generator, \\\n",
    "            validation_steps=math.ceil(len(valid_samples) / batch_size), \\\n",
    "            epochs=25, verbose=1)\n",
    "\n",
    "# Save the model and exit\n",
    "model.save('Gmodel.h5')\n",
    "print('Gmodel Saved!')\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history_object = model.fit(X_train, y_train, validation_split=0.2, shuffle=True, epochs=25)\n",
    "\n",
    "# Save the model & exit\n",
    "model.save('model.h5')\n",
    "print('Model Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_object.history.keys())\n",
    "\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-documentation",
   "metadata": {},
   "source": [
    "## 4. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python drive.py model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python drive.py Gmodel.h5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
